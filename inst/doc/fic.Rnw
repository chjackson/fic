%\VignetteIndexEntry{Focused model comparison with the fic package for R}
%\VignetteEngine{knitr::knitr}

% \documentclass[article,shortnames]{jss}
\documentclass[article,shortnames,nojss,nofooter]{jss}
\usepackage{bm}
\usepackage{tabularx}
\usepackage{graphics}
\usepackage{alltt}
\usepackage{amsmath}

<<echo=FALSE>>=
options(width=60,digits=3)
options(prompt="R> ")
library(knitr)
opts_chunk$set(fig.path="fic-")
@

\title{Different models for different purposes: focused model comparison in R}
\author{Chris Jackson <chris.jackson@mrc-bsu.cam.ac.uk>}
\Plainauthor{Christopher Jackson, MRC Biostatistics Unit}

\newcommand{\btheta}{\boldsymbol{\theta}}
\newcommand{\bbeta}{\boldsymbol{\beta}}
\newcommand{\bgamma}{\boldsymbol{\gamma}}
\newcommand{\bdelta}{\boldsymbol{\delta}}


\Abstract{
  Typical methods of model comparison pick one ``best'' model, no
  matter what the estimates from the model are used for.  ``Focused''
  model comparison, by contrast, considers that different models may
  be better for different purposes.  Different models may be preferred
  for estimating different ``focus'' quantities, functions of the
  basic parameters.

  In the ``focused information criterion'' of Claeskens and Hjort
  (2006), data are assumed to be generated by a ``wide'' model, in
  which all models we would consider are nested.  Fitting the wide
  model to the observed data, however, may give estimates that are not
  sufficiently precise.  Therefore we might accept some bias in the
  estimate in return for greater precision.  The optimal submodel for a
  particular focus is the one which minimises the mean squared error
  of the estimate of that focus from the submodel, assuming that the
  wide model is true.  \emph{[maybe other losses if can get bootstrap
    to work]}

  The \code{fic} package calculates this error, and related
  quantities, straightforwardly for any class of models fitted by
  maximum likelihood.  There are shortcuts for commonly-used model
  classes such as GLMs and parametric survival models.  Cox regression
  models are also supported \emph{[todo]} 
}


\Keywords{FIC,model comparison,AIC,BIC}

\begin{document}

\section{Introduction: principles for model comparison}

To compare a set of statistical models fitted to the same data by maximum likelihood, it is common to rank them according to some ``criterion".  For example, Akaike's information criterion (AIC, \citet{aic}) takes the form 

$$ -2\log \ell(\hat\theta ; \mathbf{x}) + 2p $$
where $\ell(\hat\theta ; \mathbf{x})$ is the maximised likelihood for the model fitted to the dataset $\mathbf{x}$, the likelihood is maximised at parameters $\hat\theta$, and $p$ is the number of parameters. 

The Bayesian information criterion (BIC, \citet{schwarz}) is

$$ -2\log \ell(\hat\theta ; \mathbf{x}) + p\log(n) $$

These two criteria are based on very different principles.  Thus they often rank models differently.  The AIC is designed to choose the model with the best predictive ability, thus it tends to favour bigger models as the sample size increases.  BIC is an approximation to Bayesian model comparison by Bayes factors, and selects the model with the highest posterior probability under an implicit weak prior (with an amount of information equivalent to one observation, see \citet{kass:wasserman} ).  If there is a ``true" model, the BIC will select it ``consistently" as the sample size increases.  In many situations there may not be a true model, and collecting more data will uncover more complexity in the process generating the data, in which case AIC may be more suitable.   See e.g. \citet{burnham:anderson:book}, \citet{claeskens:hjort:book} for more theory behind these criteria.

However both of these methods select one ``best fitting" model for a given dataset.  But that might not always be appropriate. Different models may be better for different purposes.  This is the idea behind ``focused" model comparison.   

\emph{[describe what package does, structure of paper]}


\section{Focused model comparison: principles and formulae}

Suppose the range of models we are willing to use is bounded by 

\begin{itemize}
\item a \emph{wide model}, in which all models we would use are nested, with parameters $(\btheta,\bgamma)$

\item a \emph{narrow model}, the smallest model we are willing to use, defined by setting $\bgamma=\bgamma_0$ in the wide model. 
\end{itemize}

\texttt{[list of examples.  covariate selection obvious, but lots more too]}

Suppose also that the purpose of the model is to estimate some \emph{focus} quantity, which could be any function of the basic parameters 

$$\mu = g(\btheta, \bgamma)$$

In focused model comparison, we prefer models which give better estimates of $\mu$.  A typical way to define ``better" is by the \emph{mean square error}.  The mean square error of the estimate $\hat\mu_S$ under a submodel $S$ of the wide model, compared to the true value $\mu$, is 

$$E\left\{(\hat\mu_S - \mu)^2\right\}$$ 

This expectation is calculated under the assumption that the data are generated from the wide model.   While we believe the wide model is the most realistic, we also accept that there may not be enough data to give sufficiently precise estimates of $\mu$.  Therefore we are willing to accept some bias in this estimate, in return for a smaller variance, by selecting a smaller model than the wide model.  The submodel $S$ with the lowest mean square error is the one which makes the optimal trade-off between bias and variance. 

The mean squared error $MSE_S$ under model $S$ can be decomposed as a sum of the squared bias $B_S^2$ and the variance $V_S$. 

\begin{equation}
  \label{eq:mse}
\begin{aligned}
MSE_S = E\left\{(\hat\mu_S - \mu\right)^2\}  & = \left\{E(\hat\mu_S)- \mu\right\}^2 + E\left\{(\hat\mu_S -  E(\hat\mu_S))^2\right\} \\
    & = B_S^2 + V_S
\end{aligned}
\end{equation}

Estimators for these quantities are constructed by \citet{fic} under
an asymptotic framework in which the data are assumed to be $n$
independent observations generated from the wide model, but
reparameterised so that $\bgamma = \gamma_0 + \bdelta / \sqrt{n}$.
Thus as the sample size increases, we aim to detect more subtle
departures from the narrow model.

\emph{[any more basic assumptions?]}

An obvious estimator for the bias $B_S$ is  
\( \hat{B_S} = \hat{\mu_S} - \hat{\mu_W} \),
where $\hat\mu_W$ is the estimate of the focus quantity under the wide model, which is assumed to be unbiased. 
However, \citet{fic} derive a more accurate estimator for the \emph{squared} bias as 

$$\widehat{B_S^2} = ( \hat\psi_{W} - \hat\psi_S )^2 / n$$

where 

\begin{itemize}

% bdelta is bias of par ests in narrow model on sqrtn scale  
% omegabdelta is bias of focus estimate in narrow model on sqrtn scale 

\item $\hat\psi_{W} = \hat\omega^T\hat\bdelta$ and
  $\hat\psi_S = \hat\omega^T G_S \hat\bdelta$ are estimates of
  $\omega^T\bdelta$ under the wide model and submodel respectively,
  where $\omega^T\bdelta$ is the bias of the estimate of $\sqrt{n}\mu$
  under the narrow model $N$, that is, the asymptotic mean of
  $\sqrt{n}(\hat\mu_N - \mu)$.  Thus $\omega$ acts as a linear
  transformation from the biases of the basic parameters $\bgamma$ to
  the biases of the focus parameter $\mu$.
\emph{[why not let all the $n$s cancel, and explain everything in terms of biases on the focus scale?]}

\item  $\omega$ is estimated as $\hat\omega = J_{10}J_{00}^{-1}\frac{d\mu}{d\theta} -
  \frac{d\mu}{d\gamma}$ using Taylor approximation arguments, where  $J$ is the information (inverse covariance) matrix under the wide model divided by $n$, \emph{[but the ns cancel, so could we work in terms of the info under the wide model?]} and subscripts $0$ and $1$ select the rows and columns forming the submatrices of $J$ that correspond to parameters $\btheta$ and $\bgamma$ respectively.  The partial derivatives of the focus $\mu$ are evaluated at the estimates from the wide model.

\item $G_S = \pi^T Q_S \pi  Q^{-1}$ is an estimate of the transformation that maps the wide model estimate of $\bdelta$ to the submodel $S$ estimate, where 
    $Q_S = (\pi Q^{-1} \pi^T)^{-1}$, $Q^{-1} = J_{11}$ and $\pi$ is the projection matrix consisting of 0s and 1s which maps a vector of the same length as $(\btheta,\bgamma)$ to a subvector containing the elements corresponding to submodel $S$.  \emph{[again don't the ns in the Qs cancel]}
  
\item 
$\hat\bdelta = \hat\bgamma\sqrt{n}$, where $\hat\bgamma$ is the estimate of $\bgamma$ under the wide model.
  
\end{itemize}

The estimator for the variance of $\hat{\mu}_S$ under the wide model, derived by \citet{fic}, is 
\[\hat{V} = (\hat{\tau_0}^2 + \hat{\omega}^T Q_S^0 \hat{\omega}) / n\] 

where $\hat{\tau_0}^2 /n$ estimates the variance of the narrow model focus estimate (using ``delta method" principles, $\hat\tau_0^2 = \frac{d\mu}{d\theta}^T J_{00}^{-1} \frac{d\mu}{d\theta}$), and the additional term $(\hat{\omega}^T Q_S^0 \hat{\omega}) / n$ is the increase in variance we accept by using a wider but still misspecified model $S$, with $Q_S^0 = \pi^T Q_S \pi$.

\emph{[again if we define J as info instead of info/n, then we wouldn't need to supply $n$]}

Thus we compare models on the basis of the root mean square error, estimated by
\begin{equation}
  \label{eq:rmse}
  \sqrt{\widehat{MSE_S}} = \sqrt{\widehat{B_S^2} + \hat{V_S}}
\end{equation}

\subsection{Bias-corrected MSE}
\label{sec:biascorrect}

\citet{fic} derive a further correction for the bias estimator which
is necessary when the above estimate is negative.  The adjusted squared
bias estimator is

\begin{equation}
  \label{eq:bias:adj}
  \widehat{B^{*2}} = max\left\{0,\quad \hat\omega^T (I - G_S) (\hat\bdelta \hat\bdelta^T - Q) (I - G_S)^T \hat\omega / n\right\}
\end{equation}

The corresponding estimate of the bias $B$ is $sign(\hat\psi_{W} - \hat\psi_S)\sqrt{\widehat{B^{*2}}}$, and the bias-corrected root MSE is $\sqrt{\widehat{MSE}} = \sqrt{\widehat{B^{*2}} + \hat V}$.


\subsection{Ingredients needed for the MSE calculation}

Therefore in order to calculate $MSE_S$ for a submodel $S$, we just need to know
\begin{itemize}
\item the estimates $\hat{\btheta}_W$ and $\hat{\bgamma}_W$ under the wide model,

\item the information matrix $J$ or covariance matrix of these estimates,

\item the focus function $\mu(\btheta,\bgamma)$ and its derivatives, evaluated at $\hat{\btheta}_W,\hat{\bgamma}_W$,

\item the definition of which parameters are included in submodel $S$ and which are included in the narrow model $N$. 

  \item \emph{[doubtful we also need to know n]}
\end{itemize}

\citet{fic} define the ``focused information criterion'' (FIC), which has a slightly simpler form due to excluding terms common to all submodels $S$, and is related to the MSE as
\begin{equation}
  \label{eq:fic}
  FIC_S = MSE_S - \hat\tau_0^2 + \hat\omega^T Q \hat\omega  
\end{equation}
Models with lower FIC give better estimates of the focus quantity.  However we prefer to use the (root) MSE as the model comparison statistic, due to its direct interpretation as the error of the focus estimate. 


\subsection{Average MSE over a range of focuses}

Often we want a model that performs well in a range of situations.    In covariate selection problems, for example, we might want to estimate a focus quantity accurately for a defined range of covariate values.   We might simply define the ``averaged MSE'' 

\[ AMSE = \int MSE(u) dW(u) du \] 

as a weighted average of the mean squared errors~(\ref{eq:mse}) for focuses defined by different covariate values $u$, weighted by their prevalence $W(u)$.   However \citet{claeskens:hjort:book} derived an alternative formula, so that if bias correction analogous to~(\ref{eq:bias:adj}) is required, it only needs to be performed once. 
\begin{equation}
  \label{eq:amse}
  AMSE = max(IS,0) + IIS
\end{equation}
where 
\begin{eqnarray*}
IS & = & Tr((I - G_S)(\hat\bdelta \hat\bdelta^T - Q)(I - G_S)^T A), \quad IIS = Tr(Q_S^0 A)\\
  A & = & J_{10} J_{00}^{-1}B_{00}J_{00}^{-1}J_{01} - J_{10}J_{00}^{-1}B_{01} - B_{10}J_{00}^{-1}J_{01} + B_{11} \\
B & =& \int 
\left( 
  \begin{array}{l}
  d\mu(u)/d\btheta\\
  d\mu(u)/d\bgamma
\end{array}
\right)
\left( 
\begin{array}{l}
  d\mu(u)/d\btheta\\
  d\mu(u)/d\bgamma
\end{array}
\right)
^T dW(u) \quad = \quad
\left( 
\begin{array}{ll}
  B_{00} & B_{01}\\
  B_{10} & B_{11}
\end{array}
\right)
\end{eqnarray*}

This is equivalent to AIC [ where we average over all covariates in the data ?]

\emph{[What about equivalence to AIC where the focus is the log-likelihood? ]}


\section{Example: covariate selection in logistic regression }


This example is used by \citet{claeskens:hjort:book} (Example 6.1) to illustrate the focused information criterion.  The dataset was originally presented by \citet{hosmer:lemeshow:firstedition}. Data are taken from $n=189$ women with newborn babies, and the binary outcome is whether the baby is born with a weight less than 2500g.   We build a logistic regression model to predict the outcome, but are uncertain about what covariates should be included. 

The data are provided as an object \code{birthwt} in the \code{fic} package.  This is the same as \code{birthwt} in \code{MASS} \citep{venables:ripley} with the addition of a few extra columns defining interactions and transformations as in \citet{claeskens:hjort:book}. 

The following covariates are always included (coefficient vector $\btheta$)

\begin{itemize}
\item  $x_1$ Weight of mother in kg, \code{lwtkg}
\end{itemize}


The following covariates will be selected from (coefficient vector $\bgamma$)

\begin{itemize}

\item $z_1$ age, in years, \code{age}

\item $z_2$ indicator for smoking, \code{smoke}

\item $z_3$ history of hypertension, \code{ht}

\item $z_4$ uterine irritability, \code{ui}

\item interaction $z_5 = z_1 z_2$ between smoking and age, \code{smokeage}

\item interaction $z_6 = z_2 z_4$ between smoking and uterine irritability, \code{smokeui}
\end{itemize}

Firstly the wide model, that includes all the above covariates, is defined and fitted.  

<<>>=
library(fic)
wide.glm <- glm(low ~ lwtkg + age + smoke + ht + ui + smokeage + smokeui, 
                data=birthwt, family=binomial)
@ 

The \emph{focus function} is then defined.  This should be an R function, mapping the parameters \code{par} of the wide model to the quantity of interest.   The focus can optionally have an second argument.  If supplied, this must be called \code{X}, and can be used to supply covariate values at which the focus function should be evaluated.  Here we take the probability of low birth weight as the focus, for two covariate categories: 
 
\begin{enumerate}
\item smokers with average or typical values of the other covariates.  These values are given in the order supplied when specifying the model (for smokers: intercept, \code{lwtkg}=58.24, \code{age}=22.95, \code{smoke}=1, \code{ht}=0, \code{ui}=0, \code{smokeage}=22.95, \code{smokeui}=0).

\item non-smokers with average values of the other covariates

\end{enumerate}

<<>>=
focus <- function(par, X)plogis(X %*% par)
vals.smoke <-    c(1, 58.24, 22.95, 1, 0, 0, 22.95, 0)
vals.nonsmoke <- c(1, 59.50, 23.43, 0, 0, 0, 0, 0)
X <- rbind("Smokers"=vals.smoke, "Non-smokers"=vals.nonsmoke)
@

We can illustrate these functions by calculating the probability of low birth weight, given the parameters of the fitted wide model, for each group.  This is about twice as high for smokers. 
<<>>=
focus(coef(wide.glm), X=X)
@

The \code{fic} function can then be used to calculate the mean square error of the focus for one or more given submodels.  For illustration we will compare two models, both including maternal weight, one including age and smoking, but the other including age, smoking and hypertension. 

<<>>=
mod1.glm <- glm(low ~ lwtkg + age + smoke, data=birthwt, family=binomial)
mod2.glm <- glm(low ~ lwtkg + age + smoke + ht, data=birthwt, family=binomial)
@

We supply the following arguments to the \code{fic} function.

\begin{itemize}

\item \code{wide}: the fitted wide model.  All the model fit statistics are computed using the estimates and covariance matrix from this model.   \code{fic} will automatically recognise that this is a GLM fitted by the \code{glm} function in R, and extract the relevant information. 

\item \code{inds}: indicators for which parameters are included in the submodel, that is, which elements of $(\btheta,\bgamma)$ are fixed to $\bgamma_0$.  This should have number of rows equal to the number of submodels to be assessed, and number of columns equal to $dim(\btheta) + dim(\bgamma)$, the total number of parameters in the wide model, 8 in the case of \code{wide.glm}, which includes the intercept and the coefficients of seven covariates.   It contains 1s in the positions where the parameter is included in the submodel, and 0s in positions where the parameter is excluded.  This should always be 1 in the positions defining the narrow model, as specified in \code{inds0} below.  If just one submodel is to be assessed, \code{inds} can also be supplied as a vector of length $dim(\btheta) + dim(\bgamma)$.    
  
  Note that \code{inds} indexes \emph{parameters} rather than \emph{linear model terms}, that is, in covariate selection problems where a variable is a factor with more than two levels, \code{inds} should contain separate entries for the coefficient of each factor level relative to the baseline level, not just one entry indicating the presence of the factor as a whole.   \emph{[TODO can we construct these automatically?]}

\item \code{inds0} vector of indicators for which parameters are included in the narrow model, in the same format as \code{inds}.  This can be omitted, in which case the narrow model is assumed to be given by the first row of \code{inds}.  In this case, just the first two parameters are included, the intercept and the coefficient of \code{lwtkg}.
\end{itemize}
<<>>=
inds <- rbind(mod1 = c(1,1,1,1,0,0,0,0),
              mod2 = c(1,1,1,1,1,0,0,0))
inds0 <- c(1,1,0,0,0,0,0,0)
@ 
\begin{itemize}
\item \code{focus} the focus function. 

\item \code{sub} a list of the fitted submodels to be assessed.  This is optional, and is only needed so that the model comparison statistics can be presented alongside the estimates of the focus under the submodels.  Note, if just one submodel is to be assessed, this should still be enclosed in a list, e.g. \code{list(mod1.glm)}. 

\end{itemize}

The main \code{fic} function then returns an object containing the model fit statistics and the estimate of the focus quantity for each model. 

<<>>=
sub <- list(mod1.glm, mod2.glm)
fic1 <- fic(wide=wide.glm, inds=inds, inds0=inds0, focus=focus, X=X, sub=sub)
fic1
@

The object returned by \code{fic} is a matrix containing one row for
each combination of focus covariate values indicated in the column \code{vals} and submodels indicated in the column \code{mods}.   The focus estimate is returned in the final column \code{focus}, while the remaining columns contain the following model comparison statistics:
\begin{itemize}
\item \code{FIC} The FIC as originally defined by \ref{fic} (equation~\ref{eq:fic}),
\item \code{rmse} The root mean square error of the submodel focus
  estimate, calculated assuming the wide model is true (equation~\ref{eq:rmse}),
\item \code{rmse.adj} The bias-adjusted root mean square error (Section~\ref{sec:biascorrect}),
\item \code{bias} The estimated bias $\sqrt{\widehat{B^2}}$ (which may be undefined if $\widehat{B^2}$ is negative),
\item \code{bias.adj} The adjusted bias estimate (Section~\ref{sec:biascorrect}),
\item \code{se} The standard error $\sqrt{\hat{V}}$ of
  the submodel focus estimate, calculated assuming the wide model is true.
\end{itemize}

As well as the specific covariate categories, \code{fic} calculates 
model comparison statistics which are averaged over the categories, indicated by a value of \code{ave} in the column \code{vals}.
TODO EXPLAIN WEIGHTS

Recall that \code{mod2} contains one more covariate than \code{mod1}.  For each of the
two focuses, and the average, the unadjusted and adjusted bias
estimates are lower due to the inclusion of this covariate, while the
standard error \code{se} is higher.  Given the lower \code{rmse} and
\code{rmse.adj} under \code{mod2}, the reduction in bias is deemed to
be worth the increase in uncertainty.


\subsection{Comparing a wide range of models }

In covariate selection problems, we may want to examine a broad range
of models.  The function \code{all_inds} (a wrapper around
\code{expand.grid}) creates a matrix of indicators that defines all
submodels spanned by a given wide model (here \code{wide.glm} and a
narrow model (here defined by \code{inds0}).  This
function works for all classes of model objects \code{x} for which the
\code{terms(x)} function is understood, which includes standard R
regression models such as \code{lm} and \code{glm}.  \emph{[can we use
terms.formula more generally, e.g. in flexsurvreg?]}  Factors are handled naturally. 
<<>>=
combs <- all_inds(wide.glm, inds0)
@
The resulting matrix can be used as the \code{inds} argument to
\code{fic} to calculate focused model comparison statistics for all
submodels in this example, again for a focus defined by the
probability of low birth weight at covariate values defined by
\code{X}.   However before calling \code{fic} again, we redefine
\code{combs} to exclude
models with interactions but not both corresponding main effects. 
<<>>=
combs <- with(combs,
              combs[!((smoke==0 & smokeage==1) |
                      (smoke==0 & smokeui==1) |
                      (age==0 & smokeage==1) |
                      (ui==0 & smokeui==1)),])
ficres <- fic(wide=wide.glm, inds=combs, inds0=inds0, focus=focus, X=X)
@

We can actually fit the submodels in a loop as follows, by extracting
the design matrix \code{XZ} of the wide model and removing the first
column (the intercept).  At each iteration, the
design matrix of the submodel is defined by extracting the columns of
\code{XZ} indexed by the corresponding row of \code{combs}.  The
submodel is fitted by placing this submatrix \code{XZi} on the right hand side of
the model formula passed to \code{glm} (with \code{-1} appended to
instruct \code{glm} not to include a second intercept term, since an
intercept is already included in \code{XZi}).  The list of fitted
models \code{sub} can then be passed to the \code{fic} function, so
that so that the focus estimate is displayed alongside the model
comparison statistics. 

<<warning=FALSE>>=
nmod <- nrow(combs)
sub <- vector(nmod, mode="list")
XZ <- model.matrix(wide.glm)
for (i in 1:nmod){
  XZi <- XZ[,which(combs[i,]==1)]
  sub[[i]] <- glm(low ~ XZi - 1, data=birthwt, family=binomial)
}
ficres <- fic(wide=wide.glm, inds=combs, inds0=inds0, focus=focus, X=X, 
              sub=sub)
@

Notice that some of the \code{rmse} elements of \code{ficres} are
\code{NaN}, since the first squared bias estimator $\widehat{B^2}$ is
negative.  The alternative estimate $\sqrt{\widehat{B^{*2}}}$,
\code{rmse.adj}, can simply be used in these cases.  
\emph{[this raises the question: why we
  don't just use the adjusted one all the time.  Is the unadjusted one
  better in any circumstances?]}

A comparison of many models can be illustrated by a scatterplot of
the focus estimate against the RMSE of each submodel.  The default
\code{plot} method for \code{fic} objects accomplishes this using base
R graphics: try 
<<fig.show="hide">>=
plot(ficres)
@ 
Alternatively a graph can be plotted using \code{ggplot2} if this
package is installed.   This is illustrated here. 
<<>>=
ggplot_fic(ficres)
@ 
There is one panel for each of the two covariate categories (smokers and
non-smokers) defining the focus (probability of low birth weight) and
an average over the two categories. 
The solid blue line is the focus estimate under the wide model, and
the dashed blue line is the focus estimate under the narrow model.
 An informal illustration of the uncertainty around the estimate of
 the focus quantity from
each submodel is given by the estimate $\pm 1.96 \times
\sqrt{\hat{V}}$.  

Each submodel is labelled faintly using the row
names of the matrix supplied as the \code{inds} argument to
\code{fic}.  In this case, these names were automatically
constructed by the function \code{all_inds} and contain a string of binary 0/1 
indicators for the inclusion of eight parameters.
For smokers, the narrow model (labelled \code{11000000}) and similar smaller models give
estimates of the probability of low birth weight with the 
lowest MSE, while by contrast, for non-smokers, the wide model
(labelled \code{11111111}) and similar larger
models give the most accurate estimates of the focus quantity.   Note that in this
dataset, there are 115 non-smokers and 74 smokers, thus more data
enables bigger models to be identified for non-smokers.    \emph{[discuss
average?]}


\section{Other classes of models} 

Illustrate new concepts.   Could be in separate vignettes.

\paragraph{Linear} Illustrates alternative focuses: expected outcome
at given covariate values, quantile for given covariate values.
Polynomial order selection as well as covariate selection. 
Use a well known dataset, e.g. mtcars? 

\paragraph{Multi-state} Illustrates focuses that are complicated
functions of the model parameters: package should facilitate this 

\paragraph{Survival} Increasingly flexible parametric models.
Challenge to express models as nested within each other.  Important
health economic application: restricted mean survival.

\paragraph{Skew-normal} 
Novel class of models, user-written model fitting function.  Store
est, vcov, nobs in result list.    Narrow model parameters could be in
the middle.




\subsection{Calling ``fic'' for an unfamiliar class of models }

Above, the \code{fic} function recognised the fitted model objects as GLMs, that is, objects of class \code{"glm"} returned by the \code{glm()} function in base R.
But the package can be used to calculate focused model comparison statistics for any class of models, not just the special classes it recognises. To do this, it needs to know where three things are stored inside the fitted model objects:

\begin{enumerate}
\item \code{coef}: the vector of maximum likelihood estimates $(\hat\btheta,\hat\bgamma)$,

\item \code{nobs}: the number of observations $n$ contributing to the model fit,

\item \code{vcov}: the covariance matrix of the maximum likelihood estimates, $(nJ)^{-1}$.
\end{enumerate}

Given a fitted model object called \code{mod}, the \code{fic()} function assumes by default that \code{coef(mod)}, \code{nobs(mod)} and \code{vcov(mod)} respectively return these pieces of information.  If one or more of these assumptions is not true, the defaults can be changed by supplying the argument \code{fns} to \code{fic()}, which should be a named list of three components.  Each component should be a function with one argument (the fitted model) which extracts the required information from the fitted model and returns it.  For example, the first component of the list below is a function which, when applied to a \code{glm} object, returns the maximum likelihood estimates of the regression coefficients.  (TODO better explained with a more obscure class?)

<<eval=FALSE>>=
fns <- list(coef = function(x)coef(x),
            nobs = function(x)nobs(x),
            vcov = function(x)vcov(x))
fic1 <- fic(wide=wide.glm, inds=inds, inds0=inds0, focus=focus, fns=fns, 
            X=X, sub=sub)
@


\section{Focused covariate selection in Cox proportional hazards regression} 

In a Cox regression model, time-to-event outcomes $t_i$ are observed
on individuals $i$, potentially with right-censoring.  At time $t$,
individual $i$ is assumed to have a hazard $h_i(t)$ which is
proportional to their covariate values.  We wish to select between
models that have different sets of covariates.  In the most general
``wide'' model, $h_i(t) = h_0(t)\exp(\btheta^Tx_i + \bgamma^T z_i)$.
The baseline hazard $h_0(t)$ is left unspecified, while $\btheta$ and
$\bgamma$ are estimated by maximum partial likelihood.

We compare submodels of this wide model, which include different
subsets of covariates, according to how accurately they estimate some
focus quantity $\mu = \mu(\btheta, \bgamma, H_0() | \mathbf{x}, t)$,
where $H_0()$ is the cumulative baseline hazard function.  Typical
focus quantities might depend on depend on time $t$ as well as
covariate values $\mathbf{x}$ (REF EARLIER), e.g. the probability that
a person with covariates $\mathbf{x}$ will survive $t$ years.

Again the mean square error $MSE_S = B_S^2 + V_S$ of $\mu$ of the focus
quantity under submodel $S$ is estimated as $\widehat{B_S^2} + \hat{V_S}$, using similar formulae to REF 
(CITE book, and/or paper?) 

\begin{itemize}
  
\item $\widehat{B_S^2} = (\hat\psi_W - \hat\psi_S)^2/n$, where $\hat\psi_W = (\hat\omega - \hat\kappa)^T\hat\bdelta$ and $\hat\psi_S = (\hat\omega - \hat\kappa)^TG_S\hat\bdelta$. 
  
\item $\hat{V} = \left\{\hat\tau_0^2 + (\hat\omega - \hat\kappa)^T Q_{S}^0 (\hat\omega - \hat\kappa)\right\}/n$ 

\item $\hat\omega = J_{10} J_{00}^{-1} \frac{d\mu}{d\btheta} - \frac{d\mu}{d\bgamma}$

\item $\hat\kappa(t) = (J_{10} J_{00}^{-1} F_0(t) - F_1(t))\frac{d\mu}{dH_0}$
  
\emph{[intuitive explanations of $\hat\omega$ and $\hat\kappa$, note the formulae are like before except with $\hat\omega+\hat\kappa$ instead of $\hat\omega$]}
\end{itemize}

Requires some new quantities
\[F(t) = \int_0^t G_n^{(1)} / G_n^{(0)}  dH_0(u) = \left(
  \begin{array}{l}
    F_0(t) \\
    F_1(t)
  \end{array}\right)\]
  
  where $F_0(t)$ and $F_1(t)$ have $p$ and $q$ components, and
\begin{itemize}
  \item $G_n^{(0)} = \frac{1}{n}\sum_{i=1}^n Y_i(u) \exp(x_i^T\btheta + z_i^T\bgamma)$ 
  \item $G_n^{(1)} = \frac{1}{n}\sum_{i=1}^n Y_i(u) \exp(x_i^T\btheta + z_i^T\bgamma) \left(\begin{array}{l}x_i\\z_i\end{array}\right)$ 
  \item  $Y_i(t) = I(t_i \geq t)$ at-risk indicator 
\end{itemize}

other quantities e.g. $Q_S^0,G_S,\hat\bdelta,\tau_0^2,J_{10},J_{00}$ are defined as before, using the partial likelihood where necessary instead of the likelihood. 
The asymptotic theory underlying these formulae is discussed by CITE 

hazard or cumulative hazard 



\section{Bootstrap}

Would like to 

\begin{itemize}

\item illustrate alternative losses to the mean square error 

\item use resampling to illustrate FIC principles

\end{itemize}





\section{Discussion}

Post-selection inference, model averaging 

High dimensional regression 


\bibliography{fic}

\end{document}
